{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "### LOADING LIBRARIES ###\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.decomposition import PCA\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "from sklearn.ensemble import RandomForestClassifier\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "source": [
                "### LOADING, CLEANING AND FEATURE ENGINEERING ON TRAIN AND TEST DATA ###\n",
                "\n",
                "data = pd.read_csv('/home/boris/Documents/MachineLearning/Kaggle_Titanic/train.csv')\n",
                "df_test = pd.read_csv('/home/boris/Documents/MachineLearning/Kaggle_Titanic/test.csv')\n",
                "df_test_PassId = df_test['PassengerId'] # Needed for submission\n",
                "\n",
                "#Judgment call on which columns won't offer any correlation with survival rate.\n",
                "data = data.drop(['Ticket','Cabin','PassengerId'], axis=1)\n",
                "df_test = df_test.drop(['Ticket','Cabin','PassengerId'], axis=1)\n",
                "\n",
                "#preprocess families by groups. Gives a very slight advantage\n",
                "data['FamilySize'] = data['SibSp'] + data['Parch'] + 1\n",
                "data['Singleton']=data['FamilySize'].map(lambda s:1 if s==1 else 0)\n",
                "data['SmallFamily']=data['FamilySize'].map(lambda s:1 if 2 <= s <= 4 else 0)\n",
                "data['LargeFamily']=data['FamilySize'].map(lambda s:1 if 5<= s else 0)\n",
                "\n",
                "df_test['FamilySize'] = df_test['SibSp'] + df_test['Parch'] + 1\n",
                "df_test['Singleton']=df_test['FamilySize'].map(lambda s:1 if s==1 else 0)\n",
                "df_test['SmallFamily']=df_test['FamilySize'].map(lambda s:1 if 2 <= s <= 4 else 0)\n",
                "df_test['LargeFamily']=df_test['FamilySize'].map(lambda s:1 if 5<= s else 0)\n",
                "\n",
                "#preprocess Name titles, and categorize them. There is a strong correlation between passenger title and survival rate\n",
                "Title_Dictionnary={\n",
                "\"Capt\":\"Officer\",\n",
                "\"Col\":\"Officer\",\n",
                "\"Major\":\"Officer\",\n",
                "\"Jonkheer\":\"Royalty\",\n",
                "\"Don\":\"Royalty\",\n",
                "\"Sir\":\"Royalty\",\n",
                "\"Dr\":\"Officer\",\n",
                "\"Rev\":\"Officer\",\n",
                "\"the Countess\":\"Royalty\",\n",
                "\"Mme\":\"Mrs\",\n",
                "\"Mlle\":\"Miss\",\n",
                "\"Ms\":\"Mr\",\n",
                "\"Mr\":\"Mr\",\n",
                "\"Mrs\":\"Mrs\",\n",
                "\"Miss\":\"Miss\",\n",
                "\"Master\":\"Master\",\n",
                "\"Lady\":\"Royalty\"\n",
                "}\n",
                "data['Title']=data['Name'].map(lambda name:name.split(',')[1].split('.')[0].strip())\n",
                "data['Title']=data.Title.map(Title_Dictionnary)\n",
                "\n",
                "df_test['Title']=df_test['Name'].map(lambda name:name.split(',')[1].split('.')[0].strip())\n",
                "df_test['Title']=df_test.Title.map(Title_Dictionnary)\n",
                "\n",
                "#There are 100+ rows of empty Age, which sucks because there is good correlation between Age and survival rate. I have tried different type of impute methods, but none is actually good, \n",
                "# because deleting those records always give a better prediction\n",
                "data.dropna(inplace=True)\n",
                "#I can't remove data from the test dataset unfortunately, so just the easy way, I will fill the empty data with the mean, finger crossed it will be ok\n",
                "df_test.fillna(df_test.mean(), inplace=True)\n",
                "\n",
                "# encoding title in dummy variable\n",
                "df_dummies = pd.get_dummies(data['Title'], prefix='Title')\n",
                "data = pd.concat([data, df_dummies], axis=1)\n",
                "\n",
                "df_dummies = pd.get_dummies(df_test['Title'], prefix='Title')\n",
                "df_test = pd.concat([df_test, df_dummies], axis=1)\n",
                "#Weird stuff that I don't have time to investigate, the test set doesn't contain any royaly, therefore the column didn't get created during dumiesation, and therefore my predict fails because fo the missing column. \n",
                "# So I am just manually inserting the column full of 0\n",
                "df_test['Title_Royalty']=0\n",
                "\n",
                "#Preproccess Embarked\n",
                "data.Embarked.fillna('S', inplace=True)\n",
                "# dummy encoding \n",
                "df_dummies = pd.get_dummies(data['Embarked'], prefix='Embarked')\n",
                "data = pd.concat([data, df_dummies], axis=1)\n",
                "\n",
                "df_dummies = pd.get_dummies(df_test['Embarked'], prefix='Embarked')\n",
                "df_test = pd.concat([df_test, df_dummies], axis=1)\n",
                "\n",
                "\n",
                "#Preproccess Sex\n",
                "data.Sex.fillna('M', inplace=True)\n",
                "# dummy encoding \n",
                "df_dummies = pd.get_dummies(data['Sex'], prefix='Sex')\n",
                "data = pd.concat([data, df_dummies], axis=1)\n",
                "\n",
                "df_dummies = pd.get_dummies(df_test['Sex'], prefix='Sex')\n",
                "df_test = pd.concat([df_test, df_dummies], axis=1)\n",
                "\n",
                "#Drop all obsolete column\n",
                "data = data.drop(['SibSp','Parch','FamilySize','Sex','Embarked','Title','Name'], axis=1)\n",
                "df_test = df_test.drop(['SibSp','Parch','FamilySize','Sex','Embarked','Title','Name'], axis=1)\n",
                "\n",
                "#print(data.head())"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Pclass           418\n",
                        "Name             418\n",
                        "Sex              418\n",
                        "Age              418\n",
                        "SibSp            418\n",
                        "Parch            418\n",
                        "Fare             418\n",
                        "Embarked         418\n",
                        "FamilySize       418\n",
                        "Singleton        418\n",
                        "SmallFamily      418\n",
                        "LargeFamily      418\n",
                        "Title            417\n",
                        "Title_Master     418\n",
                        "Title_Miss       418\n",
                        "Title_Mr         418\n",
                        "Title_Mrs        418\n",
                        "Title_Officer    418\n",
                        "Title_Royalty    418\n",
                        "Embarked_C       418\n",
                        "Embarked_Q       418\n",
                        "Embarked_S       418\n",
                        "Sex_female       418\n",
                        "Sex_male         418\n",
                        "dtype: int64\n",
                        "   Survived  Pclass   Age     Fare  Singleton  SmallFamily  LargeFamily  \\\n",
                        "0         0       3  22.0   7.2500          0            1            0   \n",
                        "1         1       1  38.0  71.2833          0            1            0   \n",
                        "2         1       3  26.0   7.9250          1            0            0   \n",
                        "3         1       1  35.0  53.1000          0            1            0   \n",
                        "4         0       3  35.0   8.0500          1            0            0   \n",
                        "\n",
                        "   Title_Master  Title_Miss  Title_Mr  Title_Mrs  Title_Officer  \\\n",
                        "0             0           0         1          0              0   \n",
                        "1             0           0         0          1              0   \n",
                        "2             0           1         0          0              0   \n",
                        "3             0           0         0          1              0   \n",
                        "4             0           0         1          0              0   \n",
                        "\n",
                        "   Title_Royalty  Embarked_C  Embarked_Q  Embarked_S  Sex_female  Sex_male  \n",
                        "0              0           0           0           1           0         1  \n",
                        "1              0           1           0           0           1         0  \n",
                        "2              0           0           0           1           1         0  \n",
                        "3              0           0           0           1           1         0  \n",
                        "4              0           0           0           1           0         1  \n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "### TRAIN, TEST AND EVALUATE MODEL ###\n",
                "\n",
                "y = data.Survived\n",
                "X = data.drop('Survived', axis=1)\n",
                "\n",
                "# Split data into training and test sets\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
                "                                                    test_size=0.2, \n",
                "                                                    random_state=123, \n",
                "                                                    stratify=y)\n",
                "\n",
                "# Pipelines Creation, testing different algorithm\n",
                "# 1. Data Preprocessing by using Standard Scaler\n",
                "# 2. Apply  Classifier\n",
                "\n",
                "pipeline_lr=Pipeline([('scalar1',StandardScaler()),\n",
                "                     ('lr_classifier',LogisticRegression(random_state=0))])\n",
                "\n",
                "pipeline_dt=Pipeline([('scalar2',StandardScaler()),\n",
                "                     ('dt_classifier',DecisionTreeClassifier())])\n",
                "\n",
                "pipeline_randomforest=Pipeline([('scalar3',StandardScaler()),\n",
                "                     ('rf_classifier',RandomForestClassifier())])\n",
                "\n",
                "# Lets make the list of pipelines\n",
                "pipelines = [pipeline_lr, pipeline_dt, pipeline_randomforest]\n",
                "\n",
                "best_accuracy=0.0\n",
                "best_classifier=0\n",
                "best_pipeline=\"\"\n",
                "\n",
                "# Dictionary of pipelines and classifier types for ease of reference\n",
                "pipe_dict = {0: 'Logistic Regression', 1: 'Decision Tree', 2: 'RandomForest'}\n",
                "\n",
                "# Fit the pipelines\n",
                "for pipe in pipelines:\n",
                "\tpipe.fit(X_train, y_train)\n",
                "\n",
                "for i,model in enumerate(pipelines):\n",
                "    print(\"{} Test Accuracy: {}\".format(pipe_dict[i],model.score(X_test,y_test)))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "### PERFORM PREDICTION ON TEST SET\n",
                "\n",
                "#I have ran the previous bloc manually a few time and the good old Logistic Regression always come on top. So I am going to use it\n",
                "pred = pipeline_lr.predict(df_test)\n",
                "\n",
                "output = pd.DataFrame({'PassengerId': df_test_PassId, 'Survived': pred})\n",
                "output.to_csv('/home/boris/Documents/MachineLearning/Kaggle_Titanic/my_submission_6.csv', index=False)\n",
                "print('Your submission was successfully saved!')"
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.7.4",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.7.4 64-bit ('base': conda)"
        },
        "interpreter": {
            "hash": "8856eced73ba8cb4ed9d693fc56dccaf2e6601aa98f0ee5a49275a14fb7e66a2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}